%
% This is the LaTeX template file for lecture notes for CS294-8,
% Computational Biology for Computer Scientists.  When preparing 
% LaTeX notes for this class, please use this template.
%
% To familiarize yourself with this template, the body contains
% some examples of its use.  Look them over.  Then you can
% run LaTeX on this file.  After you have LaTeXed this file then
% you can look over the result either by printing it out with
% dvips or using xdvi.
%
% This template is based on the template for Prof. Sinclair's CS 270.

\documentclass[11pt, twosides]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{xcolor}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

%
% The following commands set up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}

%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[4]{
%   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf CS 419M Introduction to Machine Learning
                        \hfill Spring 2021-22} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Lecturer: #3 \hfill Scribe: #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}
}

%
% Convention for citations is authors' initials followed by the year.
% For example, to cite a paper by Leighton and Maggs you would type
% \cite{LM89}, and to cite a paper by Strassen you would type \cite{S69}.
% (To avoid bibliography problems, for now we redefine the \cite command.)
% Also commands that create a suitable format for the reference list.
% \renewcommand{\cite}[1]{[#1]}
% \def\beginrefs{\begin{list}%
%         {[\arabic{equation}]}{\usecounter{equation}
%          \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
%          \setlength{\labelwidth}{1.6truecm}}}
% \def\endrefs{\end{list}}
% \def\bibentry#1{\item[\hbox{[#1]}]}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}
% \newcommand{\fig}[3]{
% 			\vspace{#2}
% 			\begin{center}
% 			Figure \thelecnum.#1:~#3
% 			\end{center}
% 	}
% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:

\begin{document}
%FILL IN THE RIGHT INFO.
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{2}{Loss Functions in Machine Learning}{Abir De}{180070020, 200100084, 20D070087, 180110088, 19D180002}
%\lecture{x}{Title}{Abir De}{Group y}

\section{Loss function for Image Classification}
\subsection{Problem Setup}
\begin{itemize}
    \item An Image I is represented by a vector x, x $\epsilon$ $R^d$. 2d array is collapsed into a 1d vector by well developed methods. For now we are considering only black \& white images.
    \item Label of an Image y represents the object present in the image. eg: a car, a tree or a river. If there are two classes, let's say dog and car, we can have y = 0 for dog and y = 1 for car.
    \item We consider a Dataset D to be set of images that belong to two classes either C1 or C2. $$ \mathbb{D} = \{(x_i, y_i)  | i  \in \mathbb{I}\}$$ where $x_i$ is the 1d vector representation of the image and $y_i$ $\epsilon$ $\{0,1\}$.
\end{itemize}
    

\subsection{What is Classification}
\begin{itemize}
    \item Goal of classification is to find out the labels of test/unseen images.
   \item  \textbf{Unseen Images:} The instance/image that was not revealed during the development of the ML model/algorithm.
    \item We need to design a function h($\cdot$) that will be able to give accurate class label i.e.,$$ y = h(x) \hspace{30pt}\forall \text{       } x \in \text{Test set} $$using the information from Training set.
    \item \textbf{Training set :} The set of examples (tuples ($x_{i}$,$y_{i}$) image representations along with labels) provided to the machine learning model/ algorithm at the development stage.
\end{itemize}

\subsection{How to find function h(x)}
\begin{itemize}
    \item The idea is find best $h(x) \in \mathbb{H}$, where $\mathbb{H}$ is infinite/huge set of functions such that it minimises the error.
    \item From the first principles, initial idea would be $$\min_{h(x)\in \mathbb{H}} \sum_{(x_{i},y_{i})\in \mathbb{D}}{|h(x_{i})-y_{i}|} \hspace{30pt}\text{where } y_{i} \in \{0,1\} \text{and } h(x_{i}) \in \mathbb{R}$$
    but since $y_{i} \in \{0,1\}$ only, we should look for some better answer.
    \item The new idea will be of \textbf{Penalty System} which basically means that whenever $h(x_{i})$ differs from $y_{i}$, we will add some penalty.
    The naive idea is as follows:
    \begin{itemize}
        \item if $y_{i}=0$ and $h(x_{i})= 1$, penalty = 1
        \item if $y_{i}=1$ and $h(x_{i})= 0$, penalty = 1
        \item if $y_{i}=0$ and $h(x_{i})= 0$, penalty = 0
        \item if $y_{i}=1$ and $h(x_{i})= 1$, penalty = 0
     \end{itemize}
     Keeping in mind that $h(x_{i}) \in \mathbb{R}$, we will be doing it as follows
     $$\min_{h(x)\in \mathbb{H}} \sum_{(x,y)\in \mathbb{D}}{\mathbb{I}(h(x)\ne y)}$$
    \begin{itemize}
        \item where I is \textbf{Indicator function} with values
            \begin{equation*}
    \mathbb{I}(X) =\left\{
            \begin{array}{ll}
            0 & \quad X=false\\
            1 & \quad X=true
            \end{array}
        \right.
   \end{equation*}
         \item In some sense, the above implementation is a \textbf{Hard Penalty} since we are penalising whenever $h(x)\ne y$.
     \item Moreover we have dropped "$i$" in expression for convenience and will continue to do so.
    \end{itemize}
    \item This type of function is hard to find from an infinite set of functions and work upon. So, we need to restrict the function between 0 and 1, 
    for eg. if there is a continuous function h(x) which can be transformed using a known function f($\cdot$) such that f(h(x)) gives output only as 0,1.
    then,
    $$\min_{h(x)\in \mathbb{H}} \sum_{(x,y)\in \mathbb{D}}{\mathbb{I}(f(h(x))\ne y)}$$
    in both cases we are searching over the entire space but here our work is reduced as function f(.) is ensuring that the indicator function has to only deal with a value 0 or 1 inside it.
%    We will continue to take our idea of penalty system and will improve it.\\
 %   Even after all these ideas, there is still some mismatch since $h(x) \in \mathbb{R}$ while $y \in \{0,1\}$
     \item Following the previous point, we will find  a function $f$ which can squeeze $h(x)$ to \{0,1\}.
    One such funtion $f$ is $Sign(x)$ i.e. \textbf{Signum Funtion}.
    \begin{equation*}
 Sign(x) =  \left\{
        \begin{array}{ll}
             1 & \quad x > 0 \\
             0 & \quad x = 0\\
            -1 & \quad x < 0
        \end{array}
    \right.
\end{equation*}
\begin{equation*}
 {{\frac{1+Sign(h(x))}{2}}} =  \left\{
        \begin{array}{ll}
             1 & \quad h(x) > 0 \\
            0.5 & \quad h(x) = 0\\
            0 & \quad h(x) < 0
        \end{array}
    \right.
\end{equation*}
\textbf{One very important point to note here is that our output function which will be finally used to give labels to Unseen Instances is not $h(x)$ anymore. After this transformation, our Output function will be $${{\frac{1+Sign(h(x))}{2}}}$$ }
    \item Using Sign funtion, our new optimization problem will be 
    $$\min_{h(x)\in \mathbb{H}} \sum_{(x,y)\in \mathbb{D}}{\mathbb{I}\left({\frac{1+Sign(h(x))}{2}}\ne y\right)}$$
    \textit{But its a very hard Optimization problem because} $\mathbb{H}$ \textit{is huge and working with} $\mathbb{I}$ \textit{is tedious}. \textit{So, we have to relax the conditions.}
    

\end{itemize}

\subsection{Linear Model for h(x)}

\begin{itemize}
    \item To proceed further, we will assume 
    $$h(x)= w^{T}x \hspace{30pt} \text{for some column vector w}$$
    \item So, new optimization problem is 
     $$\min_{w} \sum_{(x,y)\in \mathbb{D}}{\mathbb{I}\left({\frac{1+Sign(w^{T}x)}{2}}\ne y\right)}$$
    But optimization with Indicator function is hard.
     We will modify the problem as follows.
     $$\min_{w} \sum_{(x,y)\in \mathbb{D}}
     {\left|{\frac{1+Sign(w^{T}x)}{2}}-y\right|^{2}}$$
     But it is non differentiable due to $sign(x)$ and we will not be able to apply Calculus techniques to optimize it.\\
     \textbf{Is the below modification a good idea?}
     $$\min_{w} \sum_{(x,y)\in \mathbb{D}}
     {\left|{\frac{1+w^{T}x}{2}}-y\right|^{2}}$$\\
     \textbf{No, because $w^{T}x$ can be large which is OK but then loss will become large which is not good}  
     
     \item Its time to think of some differentiable analog to the above problem.
    One funtion which can satisfy our requirements is \textbf{Sigmoid} Function.
    \begin{equation*}
Sigmoid(x) = S(x) =\frac{1}{1+e^{-x}}\hspace{30pt} where \hspace{2pt}x \in \mathbb{R} 
\end{equation*}
Sigmoid function satisfies our requirement because it is differentiable and $S(x) \in (0,1)$ i.e. it can squeeze $h(x)$ to (0,1).\\
\textit{It has a little issue that it didn't squeeze h(x) to \{0,1\} but good point is that we can apply calculus techinques to it now.}

 Now, our problem modifies to 
$$\min_{w} \sum_{(x,y)\in \mathbb{D}}
     {\left({Sigmoid(w^{T}x)-y}\right)^{2}}$$
Note that the above optimization problem is not convex so we can do better if we can find surrogate.\\
\textbf{Reminding again that our Output function now is ${Sigmoid(w^{T}x)}$ and not $h(x)$ anymore.}
\end{itemize}
\subsection{Convex Loss function }
\begin{itemize}
    \item Convex means that if you run gradient descent then it is guaranteed to converge in the global minima.
    \item We have to find a surrogate function for $w^{T}x$ with the following properties:
    \begin{enumerate}
    \item If $w^{T}x$ is high, y is 1, then loss is 0.
    \item If $w^{T}x$ is low, y is 0 or -1, then loss is 0.
    \item The loss has to be convex with respect to w.
    \end{enumerate}
\item The way to check whether a function is convex or not is to find the double derivative and check if it is always positive with respect to w.
\item The eigen values of Hessian has to be greater than or equal to 0 for convexity with respect to \textbf{w}.
\begin{equation*}
H =
\begin{bmatrix}
\frac{\partial^2 a}{\partial w^2} 
\end{bmatrix}
\hspace{5mm} 
\lambda(H) \geq 0
\end{equation*}
\item The value of \textbf{w} at which function gives minima need not be unique, but value of function for w should be unique.
\end{itemize}
\section{Group Details and Individual Contribution}
\begin{center}
    \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Name}& \textbf{Roll Number} & \textbf{Sections}\\
    \hline
    Garaga V V S Krishna Vamsi & 180070020 & 2.1.1, 2.1.2\\
    \hline
    Vaibhav Kumar & 20d070087 & 2.1.3, 2.1.4\\
    \hline
    Abhinav Singh & 19D180002 & 2.1.5\\
    \hline
    Sristy Kushwaha & 180110088 & 2.1.3\\
    \hline
    Kavyan Lavti & 200100084 & \\
    \hline
    
    \end{tabular}
\end{center}
% Fill this part
\end{document}





